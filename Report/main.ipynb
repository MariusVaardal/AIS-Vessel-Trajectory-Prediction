{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_set(n_shifts):\n",
    "    filepath_train = r'../datasets/ais_train.csv'\n",
    "    filepath_test = r'../datasets/ais_test.csv'\n",
    "\n",
    "    # Load AIS historical data\n",
    "    train = pd.read_csv(filepath_train, sep ='|')  # Replace with your dataset\n",
    "    test = pd.read_csv(filepath_test, sep = ',')\n",
    "\n",
    "    # Preprocessing\n",
    "    train['time'] = pd.to_datetime(train['time'])\n",
    "    train.sort_values(by=['vesselId', 'time'], inplace=True)\n",
    "    train['isMoored'] = train['navstat']== 5\n",
    "    # Why are we using the complement?\n",
    "    train = train[~train['isMoored']]\n",
    "\n",
    "    test['time'] = pd.to_datetime(test['time'])\n",
    "    test.sort_values(by=['vesselId', 'time'], inplace=True)\n",
    "\n",
    "    # Feature Engineering\n",
    "    train['prev_lat'] = train.groupby('vesselId')['latitude'].shift(n_shifts)\n",
    "    train['prev_lon'] = train.groupby('vesselId')['longitude'].shift(n_shifts)\n",
    "    train['prev_speed'] = train.groupby('vesselId')['sog'].shift(n_shifts)\n",
    "    train['prev_course'] = (train.groupby('vesselId')['cog'].shift(n_shifts) / 180) - 1        # normalized\n",
    "    train['prev_rotation'] = train.groupby('vesselId')['rot'].shift(n_shifts) \n",
    "    train['prev_heading'] = (train.groupby('vesselId')['heading'].shift(n_shifts)/ 180) - 1 \n",
    "    train['hour'] = train['time'].dt.hour\n",
    "    # Could change this to is_weekend\n",
    "    train['day_of_week'] = train['time'].dt.dayofweek\n",
    "    # Adding timedelta as a feature\n",
    "    train['time_diff'] = train['time'].diff(n_shifts)\n",
    "    train['time_diff_seconds'] = train['time_diff'].dt.total_seconds()\n",
    "    # Apply the moving average function to each vessel group\n",
    "    train.dropna(inplace=True)\n",
    "    # train = train.groupby('vesselId', group_keys=False).apply(moving_average)\n",
    "    train['3_day_avg_speed'] = train.groupby('vesselId', group_keys=False).apply(\n",
    "    lambda x: x.sort_values('time').rolling('3D', on='time')['prev_speed'].mean())\n",
    "\n",
    "    # --------------------------------- prev_rot-related stuff\n",
    "    # Replace special values with NaN\n",
    "    train['prev_rotation'] = train['prev_rotation'].replace({127: np.nan, -127: np.nan, -128: np.nan})\n",
    "    train['prev_speed'] = train['prev_speed'].replace({102.3: np.nan})\n",
    "    train['prev_course'] = train['prev_course'].replace({360: np.nan})\n",
    "    train['prev_heading'] = train['prev_heading'].replace({511: np.nan})\n",
    "    train.dropna(inplace=True)\n",
    "\n",
    "    # Create binary columns for turn direction and magnitude\n",
    "    train['turn_direction'] = np.where(train['prev_rotation'] > 0, 1, 0)\n",
    "    train['turn_magnitude'] = train['prev_rotation'].abs()\n",
    "\n",
    "    # # Fill missing values (optional, using forward fill) Uses most recent non-null value from the row above.\n",
    "    # train['prev_rotation'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    train.dropna(inplace=True)\n",
    "\n",
    "    print(f\"Length of dataset after preprocessing: {len(train)}\")\n",
    "    return train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\AppData\\Local\\Temp\\ipykernel_5508\\741852009.py:35: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['3_day_avg_speed'] = train.groupby('vesselId', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset after preprocessing: 891725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\AppData\\Local\\Temp\\ipykernel_5508\\741852009.py:35: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['3_day_avg_speed'] = train.groupby('vesselId', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset after preprocessing: 891056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\AppData\\Local\\Temp\\ipykernel_5508\\741852009.py:35: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['3_day_avg_speed'] = train.groupby('vesselId', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset after preprocessing: 890379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\AppData\\Local\\Temp\\ipykernel_5508\\741852009.py:35: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['3_day_avg_speed'] = train.groupby('vesselId', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset after preprocessing: 889717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\AppData\\Local\\Temp\\ipykernel_5508\\741852009.py:35: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['3_day_avg_speed'] = train.groupby('vesselId', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset after preprocessing: 889046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\AppData\\Local\\Temp\\ipykernel_5508\\741852009.py:35: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['3_day_avg_speed'] = train.groupby('vesselId', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset after preprocessing: 888373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\AppData\\Local\\Temp\\ipykernel_5508\\741852009.py:35: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['3_day_avg_speed'] = train.groupby('vesselId', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset after preprocessing: 887700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\AppData\\Local\\Temp\\ipykernel_5508\\741852009.py:35: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['3_day_avg_speed'] = train.groupby('vesselId', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset after preprocessing: 887031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mariu\\AppData\\Local\\Temp\\ipykernel_5508\\741852009.py:35: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train['3_day_avg_speed'] = train.groupby('vesselId', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset after preprocessing: 886352\n"
     ]
    }
   ],
   "source": [
    "# train1 = make_training_set(1)\n",
    "# train2 = make_training_set(2)\n",
    "train3 = make_training_set(3)\n",
    "train4 = make_training_set(4)\n",
    "train5 = make_training_set(5)\n",
    "train6 = make_training_set(6)\n",
    "train7 = make_training_set(7)\n",
    "train8 = make_training_set(8)\n",
    "train9 = make_training_set(9)\n",
    "train10 = make_training_set(10)\n",
    "train11 = make_training_set(11)\n",
    "# train12 = make_training_set(12)\n",
    "# train13 = make_training_set(13)\n",
    "# train14 = make_training_set(14)\n",
    "\n",
    "# train = pd.concat([train8, train9, train10, train11, train12, train13, train14], ignore_index=True)\n",
    "train = pd.concat([train3, train4, train5, train6, train7, train8, train9, train10, train11], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 15\n",
      "building tree 2 of 15\n",
      "building tree 3 of 15\n",
      "building tree 4 of 15\n",
      "building tree 5 of 15\n",
      "building tree 6 of 15\n",
      "building tree 7 of 15\n",
      "building tree 8 of 15\n",
      "building tree 9 of 15\n",
      "building tree 10 of 15\n",
      "building tree 11 of 15\n",
      "building tree 12 of 15\n",
      "building tree 13 of 15\n",
      "building tree 14 of 15\n",
      "building tree 15 of 15\n",
      "building tree 1 of 15\n",
      "building tree 2 of 15\n",
      "building tree 3 of 15\n",
      "building tree 4 of 15\n",
      "building tree 5 of 15\n",
      "building tree 6 of 15\n",
      "building tree 7 of 15\n",
      "building tree 8 of 15\n",
      "building tree 9 of 15\n",
      "building tree 10 of 15\n",
      "building tree 11 of 15\n",
      "building tree 12 of 15\n",
      "building tree 13 of 15\n",
      "building tree 14 of 15\n",
      "building tree 15 of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mariu\\OneDrive\\Desktop\\moderne_maskinlæring\\venv_tdt4173\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\mariu\\OneDrive\\Desktop\\moderne_maskinlæring\\venv_tdt4173\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for Latitude: 0.059709718690664444\n",
      "Mean Absolute Error for Longitude: 0.09384142402973561\n"
     ]
    }
   ],
   "source": [
    "feats_to_include = ['prev_lat', 'prev_lon', 'prev_speed', 'prev_course','prev_rotation', 'prev_heading', 'time_diff_seconds', '3_day_avg_speed']\n",
    "X = train[feats_to_include]\n",
    "y_lat = train['latitude']\n",
    "y_lon = train['longitude']\n",
    "\n",
    "# print(train.sort_values(by=['vesselId', 'time'], inplace=False)[['vesselId', 'prev_speed', '3_day_avg_speed']])\n",
    "\n",
    "\n",
    "X_lat_train, X_lat_val, y_lat_train, y_lat_val = train_test_split(X, y_lat, test_size=0.1, random_state=42)\n",
    "X_lon_train, X_lon_val, y_lon_train, y_lon_val = train_test_split(X, y_lon, test_size=0.1, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model_lat = RandomForestRegressor(n_estimators=15, verbose=3, random_state=42, warm_start=False, criterion='squared_error')\n",
    "model_lat.fit(X_lat_train.values, y_lat_train.values)\n",
    "\n",
    "model_lon = RandomForestRegressor(n_estimators=15, verbose=3, random_state=42, warm_start=False, criterion='squared_error')\n",
    "model_lon.fit(X_lon_train.values, y_lon_train.values)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_lat_pred_val = model_lat.predict(X_lat_val)\n",
    "y_lon_pred_val = model_lon.predict(X_lon_val)\n",
    "\n",
    "# Evaluate performance on the validation set\n",
    "mae_lat = mean_absolute_error(y_lat_val, y_lat_pred_val)\n",
    "mae_lon = mean_absolute_error(y_lon_val, y_lon_pred_val)\n",
    "\n",
    "print(f'Mean Absolute Error for Latitude: {mae_lat}')\n",
    "print(f'Mean Absolute Error for Longitude: {mae_lon}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51739/51739 [20:46:08<00:00,  1.45s/it]        \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "filepath_train = r'../datasets/ais_train.csv'\n",
    "\n",
    "# Load AIS historical data\n",
    "training_data = pd.read_csv(filepath_train, sep='|')\n",
    "training_data['time'] = pd.to_datetime(training_data['time'])\n",
    "\n",
    "# Predict future positions\n",
    "def predict_future_position(id, vessel_id, time):\n",
    "    # Fetch the latest known position of the vessel\n",
    "    latest_data_points = training_data[training_data['vesselId'] == vessel_id]\n",
    "    latest_data_points_sorted = latest_data_points.sort_values(by='time')\n",
    "    \n",
    "    # Set 'time' as the index to allow for time-based rolling window\n",
    "    latest_data_points_sorted = latest_data_points_sorted.set_index('time')\n",
    "    \n",
    "    # Apply rolling window on 'sog' with a 100-day window\n",
    "    latest_data_points_sorted['3_day_avg_speed'] = latest_data_points_sorted['sog'].rolling('3D').mean()\n",
    "    \n",
    "    # Get the latest data point\n",
    "    latest_data_point = latest_data_points_sorted.iloc[-1]\n",
    "\n",
    "    # Prepare the new data for prediction\n",
    "    new_data = {\n",
    "        'prev_lat': latest_data_point['latitude'],\n",
    "        'prev_lon': latest_data_point['longitude'],\n",
    "        'prev_speed': latest_data_point['sog'],\n",
    "        'prev_course': (latest_data_point['cog'] / 180) - 1,\n",
    "        'prev_rotation': latest_data_point['rot'],\n",
    "        'prev_heading': (latest_data_point['heading'] / 180) - 1,\n",
    "\n",
    "        # Use the datetime objects for the time difference\n",
    "        'time_diff_seconds': (pd.to_datetime(time) - latest_data_point.name).total_seconds(),  # .name gives the index (time)\n",
    "        '3_day_avg_speed': latest_data_point['3_day_avg_speed'],\n",
    "    }\n",
    "\n",
    "    # Make predictions\n",
    "    return id, model_lat.predict([list(new_data.values())])[0], model_lon.predict([list(new_data.values())])[0]\n",
    "\n",
    "# Open the test file for reading and the prediction file for writing\n",
    "with open('../datasets/ais_test.csv', 'r') as f_test, open('../predictions/predictions_2.csv', 'w') as f_pred:\n",
    "    f_pred.write(\"ID,longitude_predicted,latitude_predicted\\n\")\n",
    "    for line in tqdm(f_test.readlines()[1:]):\n",
    "        id, vesselID, time, scaling_factor = line.split(',')\n",
    "        id, pred_lat, pred_lon = predict_future_position(id, vesselID, time)\n",
    "        f_pred.write(f\"{id},{pred_lon},{pred_lat}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tdt4173",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
